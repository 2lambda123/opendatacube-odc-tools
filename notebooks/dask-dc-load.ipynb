{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create/Connect to Dask Cluster\n",
    "\n",
    "In this example we just launch one locally, but you can just as well connect to existing one.\n",
    "\n",
    "## Cluster sizing\n",
    "\n",
    "Majority of the \"work\" is waiting for S3 data to arrive, so you would want to oversubscribe your cluster, i.e. have way more workers than there are CPUs, 8 workers per core is not unreasonable. \n",
    "\n",
    "Should you use threads or processes?\n",
    "\n",
    "I recommend more threads, many threads per worker process allows sharing of common data more efficiently, the downside is [GIL](https://wiki.python.org/moin/GlobalInterpreterLock), so having too many threads might become problematic. Most of the time is spent waiting for HTTP data (from S3), GIL is released during this time. Ultimately one has to experiment to see what works best for your workload. Important message is: \"don't be afraid to use more threads.\"\n",
    "\n",
    "## Use local worker pool when still debugging\n",
    "\n",
    "In the code below we launch local cluster in the same process that runs this notebook. This makes debugging any problems easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.distributed\n",
    "\n",
    "client = dask.distributed.Client(n_workers=1, \n",
    "                                 threads_per_worker=32, \n",
    "                                 processes=False, \n",
    "                                 ip='127.0.0.1')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Dask Cluster for S3 I/O\n",
    "\n",
    "1. Configure GDAL for cloud access on every worker process\n",
    "2. Check that we can obtain AWS credentials\n",
    "\n",
    "## Note on STS\n",
    "\n",
    "If using [STS](https://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html) to obtain S3 access credentials, you have to keep in mind the following:\n",
    "\n",
    "- Every worker thread will obtain its own set of credentials (first time it does IO)\n",
    "- Token expiry will cause I/O errors\n",
    "- To force credential renewal you have to call `set_default_rio_config` again on every worker\n",
    "\n",
    "Most robust and efficient way is to create a locked down set of credentials that can only read s3 buckets of interest and provision that to every worker (`~/.aws/{config|credentials}`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_setup_auto():\n",
    "    from datacube.utils.rio import set_default_rio_config, activate_from_config\n",
    "    \n",
    "    # these settings will be applied in every worker thread\n",
    "    set_default_rio_config(aws={'region_name': 'auto'},\n",
    "                           cloud_defaults=True)\n",
    "    \n",
    "    # Force activation in the main thread\n",
    "    # - Really just to test that configuration works\n",
    "    # - Every worker thread will automatically run this again\n",
    "    return activate_from_config()\n",
    "\n",
    "# Runs once on every worker process, not per worker thread!\n",
    "client.register_worker_callbacks(setup=worker_setup_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "from types import SimpleNamespace\n",
    "from datacube import Datacube\n",
    "from odc.ui import (show_datasets,\n",
    "                    to_rgba, \n",
    "                    to_jpeg_data)\n",
    "\n",
    "dc = Datacube(env='gm')\n",
    "\n",
    "cfg = SimpleNamespace(product='ls8_nbart_geomedian_annual',\n",
    "                      time='2017',\n",
    "                      crs='EPSG:3577',\n",
    "                      resolution=(-32*25, 32*25),  # 1/32 of native res\n",
    "                      dask_chunk=512,\n",
    "                      measurements=('red', 'green', 'blue'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find datasets\n",
    "\n",
    "You can supply query directly to `dc.load`, but here we are doing it in 2 steps.\n",
    "\n",
    "1. Find datasets matching query\n",
    "2. Pass datasets to `dc.load`\n",
    "\n",
    "The advantage of 2 step approach is that you can review the result of a query before committing to loading all that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dss = dc.find_datasets(product=cfg.product, time=cfg.time)\n",
    "print('Found {:,} datasets'.format(len(dss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Query Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_datasets(dss, \n",
    "              style={'fill': False, \n",
    "                     'weight': 1, \n",
    "                     'opacity': 0.3, \n",
    "                     'color': 'green'},\n",
    "              scroll_wheel_zoom=True, \n",
    "              width='400px', \n",
    "              height='400px')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Dask Array\n",
    "\n",
    "Construct lazy dask array. Below cell should execute relatively quickly as it does not load pixel data, just creates a recipe for fetching data from S3.\n",
    "\n",
    "Note that we are using `datasets=dss` instead of query parameters, which in our case is `time='2017'`. If you are not interested in previewing query result, you can just call `dc.load` directly with your query. Note also that you still have to pass in `product=` argument, it's a limitation of `dc.load`, it doesn't know that it can be extracted from `datasets=` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xx = dc.load(product=cfg.product,\n",
    "             datasets=dss,\n",
    "             output_crs=cfg.crs,\n",
    "             resolution=cfg.resolution,\n",
    "             measurements=cfg.measurements,\n",
    "             dask_chunks={'x': cfg.dask_chunk, 'y': cfg.dask_chunk})\n",
    "\n",
    "print(\"Number of chunks per band: {}x{}x{}\".format(*xx.red.data.to_delayed().shape))\n",
    "display(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually load data\n",
    "\n",
    "Note that `to_rgba` is non-dask aware function, yet it works just the same with dask arrays. It creates a `u8` RGBA image by scaling `red`/`green`/`blue` bands such that `0u16->0u8` and `clamp->255u8`, if `clamp` is not supplied maximum value across three bands is used, alpha channel is computed from `red` channel using `nodata` value.\n",
    "\n",
    "### Gotchas\n",
    "\n",
    "When calling out to non-dask aware code it is very easy to hit performance issues. Common problems are \n",
    "\n",
    "- Running out of memory\n",
    "- Loading same data more than once\n",
    "- Never sure when code will trigger data loading and when will just return another dask array without performing actual computation\n",
    "\n",
    "In the specific case of `to_rgba`, code does equivalent of this early on:\n",
    "\n",
    "```python\n",
    "r,g,b = (x.values for x in (xx.red, xx.green, xx.blue))\n",
    "```\n",
    "\n",
    "This line is almost zero cost when `xx` is fully loaded dataset, but in the case of dask arrays this triggers data loading for `red`, `green` and `blue` channels in that order. You should be able to observe that on the dask dashboard. So this particular code won't trigger double pass over the data, but on the other hand it is more likely to trigger out of memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cc = to_rgba(xx, clamp=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(data=to_jpeg_data(cc.isel(time=0).values, quality=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
