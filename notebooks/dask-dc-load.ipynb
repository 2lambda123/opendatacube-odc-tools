{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create/Connect to Dask Cluster\n",
    "\n",
    "In this example we just launch one locally, but you can just as well connect to existing one.\n",
    "\n",
    "## Cluster sizing\n",
    "\n",
    "Majority of the \"work\" is waiting for S3 data to arrive, so you would want to oversubscribe your cluster, i.e. have way more workers than there are CPUs, 8 workers per core is not unreasonable. \n",
    "\n",
    "Should you use threads or processes?\n",
    "\n",
    "I recommend more threads, many threads per worker process allows sharing of common data more efficiently, the downside is [GIL](https://wiki.python.org/moin/GlobalInterpreterLock), so having too many threads might become problematic. Most of the time is spent waiting for HTTP data (from S3), GIL is released during this time. Ultimately one has to experiment to see what works best for your workload. Important message is: \"don't be afraid to use more threads.\"\n",
    "\n",
    "## Use local worker pool when still debugging\n",
    "\n",
    "In the code below we launch local cluster in the same process that runs this notebook. This makes debugging any problems easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.distributed\n",
    "\n",
    "client = dask.distributed.Client(n_workers=1, \n",
    "                                 threads_per_worker=32, \n",
    "                                 processes=False, \n",
    "                                 ip='127.0.0.1')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Dask Cluster for S3 I/O\n",
    "\n",
    "1. Configure GDAL for cloud access on every worker process\n",
    "2. Check that we can obtain AWS credentials\n",
    "\n",
    "## Note on STS\n",
    "\n",
    "If using [STS](https://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html) to obtain S3 access credenetials, you have to keep in mind the following:\n",
    "\n",
    "- Every worker thread will obtain its own set of credentials (first time it does IO)\n",
    "- Token expiry will cause I/O errors\n",
    "- To force credential renewal you have to call `set_default_rio_config` again on every worker\n",
    "\n",
    "Most robust and efficient way is to create a locked down set of credentials that can only read s3 buckets of interest and provision that to every worker (`~/.aws/{config|credentials}`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_setup_auto():\n",
    "    from datacube.utils.rio import set_default_rio_config, activate_from_config\n",
    "    \n",
    "    # these settings will be applied in every worker thread\n",
    "    set_default_rio_config(aws={'region_name': 'auto'},\n",
    "                           cloud_defaults=True)\n",
    "    \n",
    "    # Force activation in the main thread\n",
    "    # - Really just to test that configuration works\n",
    "    # - Every worker thread will automatically run this again\n",
    "    return activate_from_config()\n",
    "\n",
    "# Runs once on every worker process, not per worker thread!\n",
    "client.register_worker_callbacks(setup=worker_setup_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from types import SimpleNamespace\n",
    "from datacube import Datacube\n",
    "from odc.ui import show_datasets\n",
    "\n",
    "dc = Datacube(env='gm')\n",
    "\n",
    "cfg = SimpleNamespace(product='ls8_nbart_geomedian_annual',\n",
    "                      time='2017',\n",
    "                      crs='EPSG:3577',\n",
    "                      resolution=(-32*25, 32*25), # 1/32 of native\n",
    "                      dask_chunk=256,\n",
    "                      measurements=('red', 'green', 'blue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dss = dc.find_datasets(product=cfg.product, time=cfg.time)\n",
    "print('Found {:,} datasets'.format(len(dss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_datasets(dss, mode='geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Dask Array\n",
    "\n",
    "Construct lazy dask array, doesn't load pixels just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xx = dc.load(product=cfg.product, \n",
    "             datasets=dss, \n",
    "             output_crs=cfg.crs,\n",
    "             resolution=cfg.resolution,\n",
    "             measurements=cfg.measurements,\n",
    "             dask_chunks={'x': cfg.dask_chunk, 'y': cfg.dask_chunk})\n",
    "\n",
    "print(\"Number of chunks per band: {}x{}x{}\".format(*xx.red.data.to_delayed().shape))\n",
    "display(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rr = xx.red.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odc.ui import to_rgba, to_jpeg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cc = to_rgba(xx, clamp=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(data=to_jpeg_data(cc.isel(time=0).values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
