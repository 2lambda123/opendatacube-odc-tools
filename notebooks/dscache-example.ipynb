{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import datacube\n",
    "from odc import dscache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Database File\n",
    "\n",
    "You can use cli tool `slurpy` to export a set of products to a file db. But if you need more control over what goes into the cache see below.\n",
    "\n",
    "## Create new file db\n",
    "Create new database, deleting any previous files that might have existed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'sample.db'\n",
    "cache = dscache.create_cache(db_name, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some datasets from Datacube\n",
    "\n",
    "We are limiting to 200 for example purposes, also there is an outstanding [issue (# 542)](https://github.com/opendatacube/datacube-core/issues/542) with `find_datasets_lazy`, it's not actually \"lazy\" the whole SQL query is processed as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(env='s2nrt')\n",
    "dss = dc.find_datasets_lazy(product='s2a_nrt_granule', limit=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write them to file db\n",
    "\n",
    "Dataset cache provides a convenience method `.tee` that accepts dataset stream on input and generates same stream on output, but also saves datasets to the file db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = cache.tee(dss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can just iterate over all datasets doing whatever other thing you needed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\n",
      "done\n",
      "CPU times: user 214 ms, sys: 21.1 ms, total: 235 ms\n",
      "Wall time: 321 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, ds in enumerate(dss):\n",
    "    if (i % 10) == 0:\n",
    "        print('.', end='', flush=True)\n",
    "print()\n",
    "print('done')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "`.tee` assumes that all datasets will be consumed, internally it breaks up dataset stream into transactions, it's not super clear what happens if you just stop half way through a transaction and never continue. Eventually transaction will be garbage collected and data written to disk, but in the meantime any writes will be blocked. So if you do exit early without consuming whole stream you should probably call `del dss` as soon as practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 143 ms, sys: 9.34 ms, total: 152 ms\n",
      "Wall time: 235 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dss = dc.find_datasets_lazy(product='s2b_nrt_granule', limit=200)\n",
    "cache.bulk_save(dss) # blocks until all are written (in one single transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,\n",
       " {'s2a_nrt_granule': DatasetType(name='s2a_nrt_granule', id_=3),\n",
       "  's2b_nrt_granule': DatasetType(name='s2b_nrt_granule', id_=4)},\n",
       " {'eo': MetadataType(name='eo', id_=1)})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.count, cache.products, cache.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin datasets into Albers Tile\n",
    "\n",
    "There is a cli tool `dstiler` that will go through all datasets in the file and bin them into various tiling regimes. Default regime is 100k side Albers tiles (same as on NCI). But there is also \"native\" for landsat scenes and \"web\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lProcessing sample.db (400 datasets)  [####################################]  100%\u001b[?25h\n",
      "Total bins: 459\n",
      "\u001b[?25lSaving  [####################################]  100%\u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!dstiler sample.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from file db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,\n",
       " {'eo': MetadataType(name='eo', id_=None)},\n",
       " {'s2a_nrt_granule': DatasetType(name='s2a_nrt_granule', id_=None),\n",
       "  's2b_nrt_granule': DatasetType(name='s2b_nrt_granule', id_=None)})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_ro = dscache.open_ro(db_name)\n",
    "cache_ro.count, cache_ro.metadata, cache_ro.products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream datasets into RAM: `.get_all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datacube.model.Dataset,\n",
       " datacube.model.DatasetType,\n",
       " datacube.model.MetadataType)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dss = list(islice(cache_ro.get_all(), 10))\n",
    "type(dss[0]), type(dss[0].type), type(dss[0].type.metadata_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access individual dataset by UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset <id=005b0ab7-5454-4eef-829d-ed081135aefb type=s2a_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2018-07-31/S2A_OPER_MSI_ARD_TL_EPAE_20180731T020636_A016215_T54JVQ_N02.06/ARD-METADATA.yaml>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_ro.get('005b0ab7-5454-4eef-829d-ed081135aefb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(459, [('albers/-07_-18', 1), ('albers/-07_-19', 2), ('albers/-07_-20', 2)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from odc.dscache.tools.tiling import parse_group_name\n",
    "\n",
    "groups = cache_ro.groups()\n",
    "len(groups), groups[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('albers/-07_-19', ((-7, -19), 'albers'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_name, count = groups[1]\n",
    "group_name, parse_group_name(group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all UUIDs for a given group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UUID('d66fd6e0-5b9f-4ab6-94b1-db1d5461f4f8'),\n",
       " UUID('fadb2197-d18e-49d0-aaa2-b8b8f799d134')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_ro.get_group(group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset documents for a given group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset <id=d66fd6e0-5b9f-4ab6-94b1-db1d5461f4f8 type=s2b_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2018-07-30/S2B_OPER_MSI_ARD_TL_EPAE_20180730T055204_A007293_T51KZB_N02.06/ARD-METADATA.yaml>,\n",
       " Dataset <id=fadb2197-d18e-49d0-aaa2-b8b8f799d134 type=s2b_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2018-07-30/S2B_OPER_MSI_ARD_TL_EPAE_20180730T055204_A007293_T51KYA_N02.06/ARD-METADATA.yaml>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cache_ro.stream_group(group_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interfacing with Datacube's `GridWorkflow.load(..)`\n",
    "\n",
    "There is a helper class that can construct `datacube.mode.Tile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-7, -19), Tile<sources=<xarray.DataArray (time: 1)>\n",
       " array([(Dataset <id=d66fd6e0-5b9f-4ab6-94b1-db1d5461f4f8 type=s2b_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2018-07-30/S2B_OPER_MSI_ARD_TL_EPAE_20180730T055204_A007293_T51KZB_N02.06/ARD-METADATA.yaml>, Dataset <id=fadb2197-d18e-49d0-aaa2-b8b8f799d134 type=s2b_nrt_granule location=s3://dea-public-data/L2/sentinel-2-nrt/S2MSIARD/2018-07-30/S2B_OPER_MSI_ARD_TL_EPAE_20180730T055204_A007293_T51KYA_N02.06/ARD-METADATA.yaml>)],\n",
       "       dtype=object)\n",
       " Coordinates:\n",
       "   * time     (time) datetime64[ns] 2018-07-30T02:01:34.939000,\n",
       " \tgeobox=GeoBox(4000, 4000, Affine(25.0, 0.0, -700000.0,\n",
       "        0.0, -25.0, -1800000.0), EPSG:3577)>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from odc.dscache.tools import DcTileExtract\n",
    "\n",
    "tiles = DcTileExtract(cache_ro)\n",
    "tile_id,_ = parse_group_name(group_name)\n",
    "\n",
    "tile = tiles(tile_id)\n",
    "\n",
    "tile_id, tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then pass `tile` object to `datacube.GridWorkflow.load(..)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
